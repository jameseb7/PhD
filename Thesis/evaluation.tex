\chapter{Evaluation}
\label{evaluation-chapter}

In this chapter, we evaluate our model and compilation strategy, using
several approaches.
\addchange{Added a sentence stating the aim of our work against which
  the strategy is evaluated}\added{The aim of our work is the
  construction of a correct compilation strategy and so our evaluation
  in this chapter focusses on establishing the correctness of the
  transformations performed in the compilation strategy.}
First, we consider what assurances can be gained from mechanisation of
the model and proofs of the compilation rules.
In addition, we compare code produced by our strategy to that produced
by icecap, using some examples to evaluate the strategy.
% TODO: more that we can add here?
We note, finally, that the process of constructing the model already
embeds important validation effort, via numerous reviews of the
standard, and close interaction with the standardisation committee,
which led to some changes to the standard.

\addchange{Clarified how evaluation relates to things previously
  presented}
\added{
For clarity, we recall what we have presented thus far. 
We have developed a model of an interpreting SCJVM, covering both the
SCJVM services and the core execution environment. 
This model has been written using Community Z Tools (CZT), so that it
is machine readable, although the nature of the checks that can be
performed on it are limited by the capabilities of CZT. 
We discuss this in more detail in
Section~\ref{mechanisation-of-models-section}. 
We have also developed a compilation strategy for translating SCJ
bytecode in the interpreter model to a representation of C code. 
Since there is not yet a sufficiently powerful automated proof
assistant for \Circus{}, the rules and their corresponding proofs are
hand-written, although some of the laws used in the proofs have been
proved using an automated proof assistant. 
The proofs of the compilation rules and the sources of the laws used
in them are discussed in Section~\ref{proofs-of-laws-section}. 
We also add that we have developed a prototype implementation of the
compilation strategy that takes in SCJ class files and outputs both C
code and the \Circus{} models resulting from the compilation strategy. 
This prototype implementation is discussed in
Section~\ref{tool-support-section} and then, in
Section~\ref{examples-section} we evaluate the strategy by applying
the prototype to some examples.
}%
\deleted{
Next, in Section~\ref{mechanisation-of-models-section}, we consider
the assurances gained from mechanisation of the models that form the
starting point of the strategy.
In Section~\ref{proofs-of-laws-section}, we discuss the proofs of the
compilation rules used in the strategy and how they provide assurances
of the correctness of the strategy.
Afterwards, in Section~\ref{tool-support-section}, we consider
mechanisation of the strategy and then, in
Section~\ref{examples-section}, we evaluate the strategy with some
examples.
}
Finally, we conclude in
Section~\ref{evaluation-final-considerations-section}.

\section{Mechanisation of Models}
\label{mechanisation-of-models-section}

% TODO: rephrase this
The correctness of our compilation strategy relies on the correctness
of the models used as input to the compilation strategy.
Their correctness relies on the inputs to the models meeting the
assumptions made in Section~\ref{compilation-assumptions-section}.
If these assumptions are not met, then the behaviour of model is not
correct and the compilation strategy cannot be applied.
For example, if the sequence of instructions in the program causes the
operand stack to overflow the maximum stack size, the invariant of
$StackFrame$ is violated and\added{ the} program's behaviour is chaotic.
Our compilation strategy cannot be applied to such a program, since no
stack slots are created beyond the maximum stack size to handle such a
situation in the strategy, and it is not clear what the expected C
code would be.

As discussed in Section~\ref{cee-validation-section}, the fact that
the models are written in CZT ensures they have correct syntax and
types.
CZT performs this checking continuously and flags up errors as they
occur, so they can be quickly corrected during the writing of the
models.

We have also performed some proofs on the Z schemas defining the
semantics of the bytecode instructions, using Z/EVES 2.4.1 with CZT as
its user interface.
There are two main groups of results.
The first is domain check proofs, ensuring partial functions are
not applied outside their domain.
These are proof obligations generated by Z/EVES, and so do not have
corresponding theorems stated.
These proofs are not required for schemas that do not directly
reference partial functions.

The second group of results is precondition proofs.
These require that a final state exists for the schema, which ensures
that the requirements of the schema are not contradictory.
Stating and proving these theorems also extracts the preconditions of
the operations, since those must be stated as assumptions of the
theorems.

The preconditions we have found include those required to avoid
operand stack overflows and underflows, that local variable indices
are within the range of the local variable array, and that
program-address updates do not go outside of the current method's
bytecode array.
These conditions are ensured by standard JVM bytecode verification,
which we assume inputs to the strategy pass.
The existence of at least one stack frame is also required for
bytecode instructions to execute, and this property is ensured by the
condition on the loop in the $Running$ action.

A further precondition required by the interpreter operations is that
the value $cs$ is such that the class and method in which a program
address occurs is unique.
This condition is required to ensure that the current class and method
can be uniquely determined from the value of $pc$.
This is required by the invariant of $InterpreterState$, but need only
be fulfilled as a precondition when a new stack frame is created,
since it can be ensured from the invariant on the initial state for
the other operations.
This condition on $cs$ is reasonable since the bytecode instructions
for each method should be at separate addresses in $bc$.

The statements of the theorems proved can be found in
Appendix~\ref{stack-frames-theorems-appendix} and
Appendix~\ref{interpreter-theorems-appendix}, with their corresponding
proofs in Appendix~\ref{stack-frames-proofs-appendix} and
Appendix~\ref{interpreter-proofs-appendix}.
We have also proved various additional lemmas in the course of
constructing these proofs.
Those which are specific to the model are listed along with the
precondition theorems in Appendix~\ref{stack-frames-theorems-appendix}
and Appendix~\ref{interpreter-theorems-appendix}.
Some of them are general facts that could be of use in other theorems,
which are listed in Appendix~\ref{additional-lemmas}.

\section{Proofs of Laws}
\label{proofs-of-laws-section}

The correctness of our compilation strategy is ensured by the
correctness of the individual compilation rules.
We prove these rules in terms of algebraic laws, whose correctness is
known.
This gives assurance that no step of the compilation strategy involves
applying a transformation that changes the semantics of the input
program.

We adopt an algebraic style of proof, in which the algebraic laws are
applied one-by-one to transform the left-hand-side of a rule into its
right-hand-side.
This ensures that the term obtained in each step of the proof is shown
to be a refinement of, or equal to, that of the previous step, by
application of a known law.
The overall proof then follows from the transitivity of refinement.
Thus, every step of the proof is justified formally and this can be
easily seen from the layout of the proof.


\addchange{Added paragraph characterising the compilation rules which
  have been proved}
\added{
Overall, there are 91 compilation rules in our strategy, all of which
are presented in Appendix A. 
Of these, we have completed hand-written proofs for 46 rules. 
In particular, we have proved all the rules used in
Algorithms~\ref{expand-bytecode-algorithm},
\ref{introduce-forward-sequence-algorithm},
\ref{introduce-loops-and-conditionals-algorithm},
\ref{refine-main-actions-algorithm},
\ref{localise-stack-frames-algorithm}
and~\ref{introduce-frameStack-assumptions-algorithm}. 
In the other algorithms, we have proved at least one rule of each
type. 
Proofs of other rules of the same type are similar. 
Note that Algorithms~\ref{epc-algorithm},
\ref{separate-complete-methods-algorithm},
\ref{pc-elimination-algorithm}, \ref{efs-algorithm},
\ref{remove-frameStack-from-state-algorithm},
\ref{redefine-method-action-excluding-return-action-algorithm},
\ref{redefine-method-action-to-include-parameters-algorithm} and
\ref{redefine-method-action-to-exclude-parameters-algorithm} only
consist of applications of algebraic laws, which we discuss below, and
references to other algorithms. 
In Algorithm~\ref{resolve-method-calls-algorithm}, we have proved
Rule~[\nameref{refine-invokestatic-rule}],
Rule~[\nameref{refine-invokevirtual-rule}] and
Rule~[\nameref{resolve-normal-method-rule}]. 
In Algorithm~\ref{remove-launcher-returns-algorithm} we have proved
Rule~[\nameref{refine-HandleAreturnEPC-empty-frameStack-rule}]. 
In Algorithm~\ref{introduce-variables-algorithm}, we have proved
Rule~[\nameref{refine-PutfieldSF-rule}] and
Rule~[\nameref{HandleAloadSF-simulation-rule}], and, finally, in
Algorithm~\ref{dro-algorithm}, we have proved
Rule~[\nameref{refine-NewObject-rule}]. 
The rules used in
Algorithms~\ref{introduce-frameClass-assumptions-algorithm}
and~\ref{introduce-operandStack-assumptions-algorithm} are assumption
distribution rules similar to those used in
Algorithm~\ref{introduce-frameStack-assumptions-algorithm}. 
Note that the rules we have not written proofs for are similar to
those already proved, and mainly concern movement of data. 
The more challenging rules that transform control flow have been
proved, particularly the loop and conditional introduction rules,
which are not applied by icecap and so cannot be checked by comparison
to icecap's output.
}

\addchange{Added numbers of laws}
\added{There are a total of 80 laws used in the proofs of the
  compilation rules.}
\deleted{The}\added{These} laws \deleted{used in the proofs} come from various sources.
\deleted{Some are existing laws taken from~\cite{oliveira2006}
and~\cite{miyazawa2012}, which have already been proved as part of
those works, and so can be safely reused.}
\added{There are 35 laws that are taken from~\cite{oliveira2006}, and
  8 laws taken from~\cite{miyazawa2012}.
  Those laws have already been proved as part of those works, and so
  can be safely reused.}
We have also used \added{3}\deleted{a few} ZRC laws
from~\cite{cavalcanti1998}, which can be applied to \Circus{} since
the semantics of ZRC are compatible with those of \Circus{}, by
Theorem 4.3 from~\cite{oliveira2006}.
Standard least-fixed-point laws, stated in~\cite{hoare1998} are also
applied to \Circus{} recursion, since it defined using
least-fixed-points\added{, and this yields a further 6 laws}.
Some \added{of the} laws follow as a trivial consequence of the
definitions given in these sources, such as
Law~[\nameref{action-intro-law}], which follows from the definition of
process refinement, which does not reference actions not used in the
main action of a process.
\added{A further 8 laws are obtained from simple combinations of the
  other laws.}

We have proved \added{20}\deleted{other} laws using the proof assistant
Isabelle~\cite{nipkow2002} with its implementation of
UTP~\cite{foster2015}.
The constructs supported by that implementation limit the types of
laws that may be proved, but we have proved several laws relating to
conditionals, assumptions, and assignment.
In the case of conditionals, we contributed an implementation of
\Circus{} conditionals to Isabelle/UTP.
This has allowed us to prove laws more general than those that have
been proved previously, since previous laws have used the fact that
conditionals can be converted to external choice, which requires that
the guards be disjoint and provide complete coverage.
We require these more general laws to perform transformation of the
$Running$ action during the elimination of program counter, since not
all program counter values have a corresponding bytecode instruction,
so we cannot ensure coverage.
Our work on this has now been integrated into Isabelle/UTP itself.

\addchange{Added some information on the lengths of proofs}
\added{
The proofs of the compilation rules occupy a total of approximately
300 pages.
The number of laws required to prove each compilation rule varies
between the compilation rules.
As an example, Rule~[\nameref{refine-invokestatic-rule}], consists of
a total of 31 applications of 16 distinct laws.
This may be regarded as a typical proof, but some rules, particularly
the assumption distribution rules, follow from specialisations of a
single law, while others, such as
Rule~[\nameref{HandleInstruction-refinement-rule}], are large proofs
involving multiple cases.
Some of the proofs make use of auxiliary lemmas that allow part of the
proof to be shared between proofs.
This is particularly the case for elimination of program counter
rules, where we must unroll the $Running$ loop as part of their
proofs.
This reuse of lemmas makes it challenging to count the total number of
laws used in these proofs, so we do not provide detailed information
on lengths of proofs here.
}

\addchange{Added mention of the number of laws used in the strategy}
\added{There are 9}\deleted{Some of the} algebraic laws \added{that}
are applied directly in our strategy\added{, in addition to the 91 compilation rules}.
\added{These}\deleted{, and} may be found at the end of
Appendix~\ref{compilation-rules-appendix}, after the compilation rules
specific to each stage of the strategy.
A full list of the algebraic laws used in this thesis, including both
those used in our compilation strategy and those used in the proofs of
the compilation rules, can be found in
Appendix~\ref{algebraic-laws-appendix}.

% discuss compilation rules and their proofs
% explain the importance of the algebraic proof style
% explain source of laws used for proofs

\section{\protect\changed{Prototype Implementation of the
    Compilation Strategy}}
\label{tool-support-section}

In addition to proving the individual compilation rules, it also is
useful to be able to automatically generate the code resulting from
the strategy in order to validate it.
This allows for consideration of the issues involved in handling
actual SCJ programs and shows how the strategy as a whole fits
together to produce the final code.
It also facilitates the consideration of examples, which provide
additional validation of the strategy.

\addchange{Added mention of C code generation by prototype} We have
thus created a simple prototype to transform SCJ class files to\added{
  the} corresponding \Circus{} models generated by the
strategy\added{, and their corresponding C code}.
This prototype is written in Java, using the Apache bytecode emulation
library for reading class files so that real output from the standard
Java compiler can be used directly.
It outputs the \Circus{} code for the $CThr$ process that results from
applying the compilation strategy to the input files.
We focus on this part of the C code model, and the first two stages of
the strategy that generate it, as it is quite complex and so most
benefits from review of the code produced.
\added{The C code that corresponds to the $CThr$ process is also
  generated by our prototype, by traversing the \Circus{} abstract
  syntax tree to output the C code corresponding to each \Circus{}
  construct.}

The data refinement of memory is comparatively simple, since it just
involves collecting the fields for each class and producing the
corresponding \Circus{} code from the strategy.
Its correctness is sufficiently ensured by the correctness of the
compilation rules, so we do not handle it in our prototype.

\begin{figure}[p]
  \begin{center}
    \begin{tikzpicture}
      % \begin{class}{ClassFileModelConverter}{0,0}
      %   \operation{+ \uline{main(args : String[])}}
      % \end{class}

      \umlclass{Model}{
        $\cdots$
      }{
        + toModelString() : String \\
        + doEliminationOfProgramCounter() : ThrCFModel \\
        - methods() : HashSet\textless{}FullMethodID\textgreater{} \\
        - allMethodsSeparated(newModel : ThrCFModel) : boolean \\
        $\cdots$
      }

      \umlclass[x=-3.5cm,y=-3.2cm]{ClassModel}{$\cdots$}{$\cdots$}

      \umlclass[x=3.5cm,y=-3.2cm,type=abstract]{BytecodeModel}{}{}

      \umlclass[alias=aconstnull,x=8cm,y=-2cm]{ACONST\_NULL}{$\cdots$}{$\cdots$}
      \node at (8cm,-3.8cm) {\Huge  $\vdots$};
      \umlclass[x=8cm,y=-6cm]{RETURN}{$\cdots$}{$\cdots$}

      \umlinherit[geometry=-|-]{aconstnull}{BytecodeModel}
      
      \umlinherit[geometry=-|-]{RETURN}{BytecodeModel}

      \umluniaggreg[geometry=|-,anchor1=-110,arg2=classes,mult2=0..*,pos2=1.4,arm2=-0.1cm]{Model}{ClassModel}
      \umluniaggreg[geometry=|-,anchor1=-90,arg2=bytecodes,mult2=0..*,pos2=1.6,arm2=-0.1cm]{Model}{BytecodeModel}

      \umlclass[x=0cm,y=-7.8cm]{ThrCFModel}{
        $\cdots$
      }{
        + addMethod(name : FullMethodID, actions : CircusAction[]) \\
        + toModelString() : String \\
        + doEliminationOfFrameStack() : CThrModel \\
        - getReturnAction(CircusAction[] actions) : CircusAction \\
        - introduceReturnActions(actions : CircusAction[], \\
        $\t1$ returnAction : CircusAction) : CircusAction[] \\
        - returnActionDist(actions : CircusAction[]) : CircusAction[] \\
        %- eliminateNewStackFrame(actions : CircusAction[], \\
        %$\t1$ methodReturnsValue : HashMap\textless{}FullMethodID, Boolean\textgreater{}) : CircusAction[] \\
        %- eliminateVarBlocks(actions : CircusAction[]) : CircusAction[] \\
        $\cdots$
      }

      \umluniaggreg[geometry=|-|,anchor1=130,arg2=classes,mult2=0..*,pos2=2.8,arm1=0.4cm]{ThrCFModel}{ClassModel}

      \umlclass[x=0cm,y=-13cm,type=abstract]{CircusAction}{}{
        \umlvirt{+ expandWithClassInfo(classInfo : ClassModel) : CircusAction} \\
        \umlvirt{+ doEFSDataRefinement(stackDepth : int) : CircusAction[]} \\
        \umlvirt{\added{+ toModelString(indentLevel : int) : String}} \\
        \umlvirt{\added{+ toCCode(indentLevel : int) : String}} \\
        $\cdots$
      }

      \umlclass[alias=handleaconstnullepc,x=7cm,y=-16.1cm]{HandleAconst\_nullEPC}{
        $\cdots$
      }{
        $\cdots$
      }
      \node at (7cm,-17.6cm) {\Huge  $\vdots$};
      \umlclass[x=7cm,y=-19.5cm]{Assignment}{
        $\cdots$
      }{
        % + getVar() : String \\
        % + getExpr() : String() \\
        $\cdots$
      }

      \umlinherit[geometry=-|,anchor2=-30]{handleaconstnullepc}{CircusAction}
      
      \umlinherit[geometry=-|,anchor2=-30]{Assignment}{CircusAction}

      \umluniaggreg[geometry=|-|,anchor1=-90,arg2=methodActions,mult2=0..*,pos2=2.8]{ThrCFModel}{CircusAction}

      \umlclass[x=0cm,y=-17.5cm]{CThrModel}{
        $\cdots$
      }{
        + toModelString() : String \\
        \added{+ toCCode() : String} \\
        $\cdots$
      }

      \umluniaggreg[geometry=|-|,anchor1=90,arg2=methodActions,mult2=0..*,pos2=2.8]{CThrModel}{CircusAction}
      
      % \aggregation{Model}{classes}{0..*}{ClassModel}
      % \aggregation{Model}{bytecodes}{0..*}{BytecodeModel}
    \end{tikzpicture}
  \end{center}
  \caption{Class diagram for our implementation of the compilation
    strategy}
  \label{implementation-class-diagram-figure}
\end{figure}

To ensure we get the most benefit from our prototype, we follow the
strategy and the form of the compilation rules as closely as possible
in its design, shown in
Figure~\ref{implementation-class-diagram-figure}.
Our implementation of the compilation strategy validates our reasoning
in designing it, since the code generated for the examples has the
expected form matching that of the icecap compiler.

Some of the classes used in our implementation and the relationships
between them are shown in
Figure~\ref{implementation-class-diagram-figure}. 
Our prototype begins by reading each input class file and extracting
the information into \texttt{ClassModel} and
\texttt{BytecodeModel} classes.
\texttt{ClassModel} represents the $Class$ type from our model and
makes available all the information represented in that type.
\texttt{BytecodeModel} is an abstract class whose subclasses represent
individual bytecode instructions; it represents the $Bytecode$ type
from our model.
The set of \texttt{ClassModel} structures and array of
\texttt{BytecodeModel}s are collected together into a \texttt{Model},
representing the inputs to the compilation strategy.

The application of the first stage of the compilation strategy to a
\texttt{Model} is initiated by invocation of its
\texttt{doE\added{l}iminationOfProgramCounter()} method.
This returns a \texttt{ThrCFModel} object, which represents the
$ThrCF$ process generated from the inputs represented by the
\texttt{Model}.
The \texttt{doE\added{l}iminationOfProgramCounter()} method applies each step
of Algorithm~\ref{epc-algorithm}.
It begins by replacing each bytecode instruction with the \Circus{}
actions that result from applying bytecode expansion to it, as
described in Algorithm~\ref{expand-bytecode-algorithm}.
We represent \Circus{} actions by subtypes of an abstract class
\texttt{CircusAction}.
These subtypes represent both general \Circus{} constructs such as
variable blocks, conditionals and assignment, and references to
specific actions in our model, such as the $Handle{*}EPC$ actions.

The sequences of actions produced by bytecode expansion are placed
into an array of arrays of \texttt{CircusAction}s, representing the
branches of the choice over $pc$ in $Running$.
We test the types of the actions in these sequences to check if they
match the compilation rules of the strategy, and update the sequence
of actions in a branch accordingly, in order to perform the
introduction of sequential composition
(Algorithm~\ref{introduce-forward-sequence-algorithm}), introduction
of loops and conditionals
(Algorithm~\ref{introduce-loops-and-conditionals-algorithm}), and
method resolution (Algorithm~\ref{resolve-method-calls-algorithm}).

We also construct a control-flow graph, which we use to guard the
application of some rules as indicated in the strategy, and which is
reconstructed after the application of a compilation rule.
The sequence of actions corresponding to\added{ the} entry point of a method whose
control-flow graph consists of a single node are added to the
\texttt{ThrCFModel} during method separation
(Algorithm~\ref{separate-complete-methods-algorithm}), with their $pc$
assignments removed when they are added, to produce the result of
Algorithm~\ref{pc-elimination-algorithm}.
The refine main actions step
(Algorithm~\ref{refine-main-actions-algorithm}) operates on the
$Started$ and $MainThread$ actions, which have a known form, so we
simply output the form resulting from this step, instantiated with the
method names collected in the strategy.

The application of the elimination of frame stack stage to the
\texttt{ThrCFModel} is performed by its
\texttt{doEliminationOfFrameStack()} method, which returns a
\texttt{CThrModel} representing the $CThr$ process generated after
this stage.
In our implementation we apply the rules of this stage by traversing
the actions of each method, checking for actions that match the form
of the rules.
Rules that operate on sequences of more than one rule are applied by
private methods of \texttt{ThrCFModel}, whereas those that affect only
a single action are applied by methods of the \texttt{CircusAction}
classes.
We group together the application of similar rules in some of these
methods.

The removal of launcher returns
(Algorithm~\ref{remove-launcher-returns-algorithm}) is performed by
first obtaining the return action with a \texttt{getReturnAction()}
method, which corresponds to the \Call{ReturnAction}{} function
referenced on line~\ref{algorithm-determine-return-action} of
Algorithm~\ref{remove-launcher-returns-algorithm}.
The return action is then introduced after infinite loops by a
method \texttt{introduceReturnActions()}, which performs the
exhaustive application of Law~[\nameref{rec-action-intro-law}] on
line~\ref{algorithm-introduce-infinite-loop-returns}, and distributed
using a method \texttt{returnActionDist()}, which performs the
exhaustive application of Rule~[\nameref{conditional-dist-rule}] on
line~\ref{algorithm-distribute-return}.
The remainder of this step is upon the $ExecuteMethod$, $Started$ and
$MainThread$ actions, whose forms are known, so we simply output the
resultant forms for them at the end of the application of the
strategy.
The return actions within the body of a method are refined to the
corresponding data operations by this step so we take them to refer to
those data operations in subsequent steps.
Although the return actions are distributed outside the method actions
in this step, they are moved back inside the method actions in the
next step, so we do not perform this moving in the implementation.

During the localise stack frames step
(Algorithm~\ref{localise-stack-frames-algorithm}), the data refinement
on line~\ref{algorithm-remove-currentClass-data-refinement} of
Algorithm~\ref{localise-stack-frames-algorithm} only affects the
definition of the process' data operations, not the \Circus{} code for
the methods of our program, so it does not need to be explicitly
performed in our implementation.
We instead begin localising the stack frames by calculating the number
of arguments for each method as specified on
lines~\ref{algorithm-numArgs-declaration}
to~\ref{algorithm-static-args-check-end} of
Algorithm~\ref{localise-stack-frames-algorithm}, and then refining
each method by adding parameters as specified by
Rule~[\nameref{arguments-introduction-rule}] and a $stackFrame$
variable block as specified by
Rule~[\nameref{HandleReturnEPC-stackFrame-introduction-rule}].
These are added directly to each method's actions, since the
parametrised block is moved inside the method by the procedure called
on line~\ref{algorithm-redefine-method-action-to-include-parameters}.
After this, the $InterpreterNewStackFrame$ operations are eliminated
from the body of each method by a method
\texttt{eliminateNewStackFrame()}, because they are moved inside
the methods whose references follow them and refined to stack frame
initialisation operations.

In the introduce variables step, the \texttt{expandWithClassInfo()}
method of \texttt{CircusAction} applies the rules on
lines~\ref{algorithm-apply-refine-PutfieldSF}
to~\ref{algorithm-apply-refine-NewSF} of
Algorithm~\ref{introduce-variables-algorithm}, which make use of
information on the value of $frameClass$.
The data refinement on line~\ref{algorithm-local-data-refinement} is
performed by \texttt{doEFSDataRefinement()}, passing the depth of the
operand stack at each point in the method based on the rules in
Algorithm~\ref{introduce-operandStack-assumptions-algorithm}.
Finally, \texttt{eliminateVarBlocks()} applies the rules on
lines~\ref{algorithm-apply-eliminate-value1-value2-conditional-rule}
to~\ref{algorithm-argument-variable-elimination} of
Algorithm~\ref{introduce-variables-algorithm}, which eliminate extra
variable blocks around various constructs.
The removal of the $frameStack$ from the state
(Algorithm~\ref{remove-frameStack-from-state-algorithm}) is trivial
and has no effect on the method actions so there is nothing to be done
for it in our implementation.

The \Circus{} model resulting from this stage is extracted as a
\texttt{String} from the \texttt{CThrModel}\added{, using its
  \texttt{toModelString()} method,} and written to an output file.
\addchange{Added explanation of how \protect\Circus{} and C code is
  output}%
\added{The \texttt{toModelString()} method of \texttt{CThrModel} calls
  the \texttt{toModelString()} method of each \texttt{CircusAction} to
  traverse the \Circus{} syntax tree and output the \LaTeX{}
  representation of each \Circus{} construct.
  If the corresponding C code is desired, the \texttt{toCCode()}
  methods of \texttt{CThrModel} and \texttt{CircusAction} are used
  instead to output the C code representation of each \Circus{}
  construct.
  A C header file is also output containing struct definitions and
  function prototypes for operations defined in the launcher, to
  ensure that all the definitions required by the C code are
  available.}

As our prototype is just for the purposes of validating the strategy,
we have not performed a direct formal verification of its
implementation.
However, since we have applied the compilation rules in the
implementation in a way that matches the form of the rules in the
strategy, which are proved, we are confident of its correctness.
The correctness of the implementation is further validated by loading
the\added{ \Circus{} code} output\deleted{ of}\added{ from} the
prototype into CZT to ensure that it is well-formed, and checking the
output to ensure it has the expected form.

\addchange{Added explanation of how well-formedness of the C code
  output by our strategy is established}
\added{The well-formedness of the C code output from our prototype is
  shown by the fact that it compiles without errors or warnings on GCC
  7.3.0, using the command \mbox{\texttt{gcc -c -Wall -pedantic}}.
  The choice of warning flags for this compilation matches those used
  by icecap when launching an icecap program from within Eclipse.
  We note that our code can only be compiled, and not linked, as
  creating an SCJVM services implementation to link to our program
  code is outside the scope of this work.}

% TODO: rephrase to account for stuff moved to next section
There have been various considerations raised in producing this
prototype. 
One consideration is that of how to represent the class, field,
and method identifiers used in the bytecode.
In the model these are represented by given sets, since their
representations do not matter provided they can be distinguished from
one another and information necessary to the operation of the strategy
(specifically, the number of arguments to a method identifier and
whether it denotes an instance initialisation method) can be gleaned
from them.
For simplicity, we just use the identifier strings supplied in the
input Java class files, concatenating method and field names with
their type signatures, and removing or replacing characters that are
not valid in \Circus{} identifiers.
% However, many of these identifiers are quite long, since they use
% fully qualified class names, so we generally shorten them when
% presenting examples, provided the identifiers are still unique.
% The shortenings performed are generally elimination of packages names
% from class names, and removal of type signatures from method
% identifiers when two methods do not have the same name (initialisation
% method identifiers have a shortened version of their class name
% prepended to distinguish them).

Since we apply the compilation rules in our implementation as
prescribed in our strategy, we can observe how the individually
correct compilation rules fit together.
It has highlighted the need to consider the extent of
variable blocks.
In particular, the loop and conditional introduction rules must match
the variable block introduced by the expansion of the
\texttt{if\_icmple} bytecode instruction. 

We also found that Rule~[\nameref{resolve-normal-method-rule}] must
extend the $poppedArgs$ variable block to cover the reference to the
method action it introduces, in order to match the combination
of\added{ the} $IntepreterNewStackFrame$ operation and method action
reference in Rule~[\nameref{arguments-introduction-rule}].
In addition, it revealed that the return action must be distributed
outside of the variable blocks surrounding conditionals in
Rule~[\nameref{conditional-dist-rule}].
The form of the methods resulting from the elimination of program
counter also made clear the need for $Poll$ actions before $Running$
in $Started$ and $MainThread$, in order to match method calls
introduced in the body of methods.

All these considerations have been taken into account in the strategy
presented in the previous chapter.
In the next section, we discuss some examples whose compilation we
have automated using our prototype. 
We focus on the generated code, and its relation to icecap results.

% discuss mechanisation of strategy
% explain considerations surrounding bytecode subset
% explain what confidence this brings

\section{Examples}
\label{examples-section}

In this section, we evaluate the strategy by considering some examples
of SCJ programs.
We compare the code generated from the prototype implementation of the
compilation strategy to that resulting from the icecap HVM for each of
the examples.
The examples we \changed{have chosen} are taken from those developed
during the high-integrity Java applications using \Circus{}
project~(which may be found
at~\url{www.cs.york.ac.uk/circus/hijac/case.html}).

We particularly focus on SCJ Level 1 examples that illustrate some of
the main features of SCJ.
These examples cover the full range of bytecode instructions in our
subset, and include various examples of loop and conditional
constructs to test the strategy.
There are three examples we discuss.
The first is \texttt{PersistentSignal}, discussed in
Section~\ref{persistent-signal-subsection}, which demonstrates SCJ
scheduling behaviour.
The second is \texttt{Buffer}, discussed in
Section~\ref{buffer-subsection}, which demonstrates SCJ memory
behaviour.
Finally, the third example, \texttt{Barrier}, \added{which demonstrates a
common synchronisation pattern in real-time systems,} is discussed in
Section~\ref{barrier-subsection}.

We have run the examples through both our prototype and the icecap
compiler.
\addchange{Moved discussion of issues with the strategy fixed while considering examples earlier}%
\moved{%
\added{While running the examples through our prototype,} various
issues with the compilation strategy and our prototype have been
identified and fixed\deleted{ while considering these examples}.
The first is that the examples make use of bytecode instructions that
are not in the representative subset of instructions described in
Section~\ref{cee-bytecode-subset-subsection}.
However, since this is a representative subset, the strategy can be
easily expanded to handle the missing instructions by analogy to the
instructions in the subset.
For example binary operation bytecodes can have their semantics
defined in a similar way to \texttt{iadd}, with compilation rules to
handle them similar to those for \texttt{iadd} (and their
corresponding proofs similar).
\added{These extra rules are implemented in our prototype.}
Conditional instructions can be handled in a similar way to
\texttt{if\_icmple} during bytecode expansion, and subsequently
handled by existing compilation rules.
Also, while we did not consider \texttt{long} values in our strategy,
we have implemented handling of operations on \texttt{long} values in
our prototype, operating on pairs of variables and stack slots.

Array instructions \deleted{are slightly more challenging to replicate
  in the strategy, but} can be represented in programs using classes
that contain the individual slots of the array as fields. 
These fields can then be accessed using methods that select the
appropriate field with conditionals over an array index.
A full implementation would replace these method calls with
specialised communications with the struct manager, which would handle
them using C arrays.
Although this would require changes to the object/struct manager, it
would be \deleted{relatively} simple in terms of the strategy as the
structure of the arrays would not change during compilation and very
little would need to be performed on the instructions in the
interpreter.

We \added{have} also found that $poppedArgs$ variable blocks around
special method calls are not eliminated in
Algorithm~\ref{introduce-variables-algorithm}, although variable
blocks for normal methods are handled by
Rule~[\nameref{method-parameter-introduction-rule}] and
Rule~[\nameref{poppedArgs-sync-elim-rule}].
Eliminating $poppedArgs$ around special methods requires rules similar
to these rules to handle each individual special method.
The rules required are simple rules to substitute the value of
$poppedArgs$ into the body of the action and then eliminate the
variable block with the initialisation of $poppedArgs$.
}

\begin{figure}[t]
  \centering
  \begin{tikzpicture}
    \path (0,0) -- (0,5)
    node[pos=0.0] (bottom) {}
    node[pos=0.3] (machineBoundary) {}
    node[pos=0.6] (codeBoundary) {}
    node[pos=1.0] (top) {};

    \path (0,0) -- (10,0)
    node[pos=-0.3] (left) {}
    node[pos=0.00] (preCompilationLeft) {}
    node[pos=0.25] (preCompilationRight) {}
    node[pos=0.35] (JavaRight) {}
    node[pos=0.65] (CLeft) {}
    node[pos=0.75] (postCompilationLeft) {}
    node[pos=1.00] (postCompilationRight) {}
    node[pos=1.00] (right) {};

    \draw (bottom -| preCompilationLeft) rectangle (top -| preCompilationRight);
    \path (bottom -| preCompilationLeft) rectangle (machineBoundary -| preCompilationRight) coordinate[pos=0.5] (PreVM);
    \draw[dashed] (machineBoundary -| preCompilationLeft) rectangle (machineBoundary -| preCompilationRight);
    \path (machineBoundary -| preCompilationLeft) rectangle (codeBoundary -| preCompilationRight) coordinate[pos=0.5] (PreAPI);
    \draw (codeBoundary -| preCompilationLeft) rectangle (codeBoundary -| preCompilationRight);
    \path (codeBoundary -| preCompilationLeft) rectangle (top -| preCompilationRight) coordinate[pos=0.5] (PreCode);

    \node[align=center] at (PreVM) {OS and\\SCJVM\\services};
    \node[align=center] at (PreAPI) {SCJ API};
    \node[align=center] at (PreCode) {SCJ\\Program};

    \draw (bottom -| left) -- (bottom -| preCompilationLeft);
    \draw (codeBoundary -| left) -- (codeBoundary -| preCompilationLeft);
    \path (bottom -| left) -- (codeBoundary -| preCompilationLeft) coordinate[pos=0.5] (SCJVM);
    \node[align=center] at (SCJVM) {SCJ\\Infrastructure\\Implementation};

    \draw (top -| preCompilationRight) -- (top -| JavaRight);
    \draw (machineBoundary -| preCompilationRight) -- (machineBoundary -| JavaRight);
    \path (machineBoundary -| preCompilationRight) -- (top -| JavaRight) coordinate[pos=0.5] (JavaCode);
    \draw (machineBoundary -| JavaRight) -- (top -| JavaRight) coordinate[pos=0.5] (JavaCodeMid);
    \node[align=center] at (JavaCode) {Java\\Code};
    
    \draw (bottom -| postCompilationLeft) rectangle (top -| postCompilationRight);
    \path (bottom -| postCompilationLeft) rectangle (machineBoundary -| postCompilationRight) coordinate[pos=0.5] (PostVM);
    \draw[dashed] (machineBoundary -| postCompilationLeft) rectangle (machineBoundary -| postCompilationRight);
    \path (machineBoundary -| postCompilationLeft) rectangle (codeBoundary -| postCompilationRight) coordinate[pos=0.5] (PostAPI);
    \draw (codeBoundary -| postCompilationLeft) rectangle (codeBoundary -| postCompilationRight);
    \path (codeBoundary -| postCompilationLeft) rectangle (top -| postCompilationRight) coordinate[pos=0.5] (PostCode);

    \node[align=center] at (PostVM) {OS and\\SCJVM\\services};
    \node[align=center] at (PostAPI) {C Code\\of SCJ API};
    \node[align=center] at (PostCode) {C Program};

    \draw (top -| CLeft) -- (top -| postCompilationLeft);
    \draw (machineBoundary -| postCompilationLeft) -- (machineBoundary -| CLeft);
    \path (machineBoundary -| postCompilationLeft) -- (top -| CLeft) coordinate[pos=0.5] (CCode);
    \draw (machineBoundary -| CLeft) -- (top -| CLeft) coordinate[pos=0.5] (CCodeMid);
    \node[align=center] at (CCode) {C\\Code};
    
    \draw[-latex] (JavaCodeMid) -- node[above] {compilation} (CCodeMid);
    
  \end{tikzpicture}
  \caption{The relation of an SCJ program to the SCJ infrastructure
    in compilation}
  \label{program-infrastructure-compilation-figure}
\end{figure}

\addchange{Changed discussion of Figure 6.2 to better explain why we
  consider the program methods but not the API methods in the code
  comparison}
\added{
After generating the code for each of our examples, we have evaluated
the examples by comparing the code generated by our prototype for each
of the program methods to that generated by icecap.
We focus on the methods of the example programs themselves, rather
than the methods of the SCJ API, which are compiled along with the
program code.
}%
\deleted{%
  Our main focus here is validating the compilation of the SCJ
  programs themselves, which is separate from the SCJ infrastructure
  it depends on.
  Both icecap and our strategy, however, operate on complete SCJ
  programs, including the Java code that implements the SCJ API.
}%
\changed{This can be seen in}
Figure~\ref{program-infrastructure-compilation-figure}, which
\changed{shows} the structure of an SCJ program in relation to the
infrastructure and compilation.

\added{
The SCJ program depends on the SCJ infrastructure implementation,
which consists of an SCJ API implementation, possibly written in Java,
and the OS and SCJVM services, written in some native language.
Only the parts written in Java are subject to compilation, so the OS
and SCJVM services are not included in the compilation.
}%
How much of the SCJ API implementation is written in Java, and hence
included in the code that undergoes compilation, depends upon the
\changed{OS and SCJVM services}.
These are generally accessed through native method calls in Java code,
but are usually implementation-defined and not visible to end-users,
as indicated by the dashed line in
Figure~\ref{program-infrastructure-compilation-figure}.

% The infrastructure in our model is accessed via special methods
% representing these native methods, but our model of the SCJ
% infrastructure is intended to model a wider part of the SCJ
% infrastructure than that which icecap implements in C.
In our model, native methods are represented by special methods, which
are called using channels, rather than bytecode invoke instructions.
Our model of the infrastructure covers the elements in the SCJ
standard. 
In icecap, however, some are implemented in Java, and some are
implemented in C. 
So, when compared to our compilation, icecap deals with more Java code
than we do.
To account for these differences when passing the examples through the
compilation strategy, we provide a small implementation of part of the
SCJ API, linking the SCJ code of the examples to the SCJVM via the
special methods in our model.

\added{
The SCJ API implementation code passed to our prototype is thus
different from the SCJ API implementation used by icecap, although the
program code is the same, since the methods of the SCJ API are the
same and it is only their implementation that differs.
}%
\deleted{%
This does not affect the correctness of the examples or the validity
of the comparisons, since the SCJ API is the same for both icecap and
our implementation; it is only the implementation of the API that
differs.
}%
We thus\added{, as already mentioned,} focus on the program code in
our evaluation of the examples.
\deleted{%
After compilation, the code corresponding to the methods of the SCJ
API is still separate from the code corresponding to the methods of the SCJ
program, as indicated in
Figure~\ref{program-infrastructure-compilation-figure}, so they can be
easily distinguished.
}%
Ensuring the correctness of the API implementation is a separate
issue, work on which has begun in~\cite{freitas2016}.

% In order to best compare the code generated from the examples, we have
% tried to keep the code passed to icecap and our strategy as similar as
% possible but, since much of the SCJ infrastructure relies on direct
% interaction with the SCJVM to provide features such as scheduling and
% memory management, we have provided our own library covering part of
% the SCJ API.
% Our implementation of the SCJ API is generally a quite thin layer,
% providing the required classes of SCJ and linking the SCJ code to the
% SCJVM via the special methods in our model, which represent native
% methods in the Java code.
% Much of the SCJ infrastructure is already represented in our model by
% the $Launcher$ and SCJVM services, so it is not necessary to provide
% code for it in our SCJ API implementation.
% The API we provide is the same as that for icecap, so the compilation
% results for the code of the examples themselves are similar.

\addchange{Added explanation of how code is compared, and structure of
  the discussion of the examples}
\added{
In comparing the methods of the program, we have noted the
similarities and differences between our code and the code generated
by icecap, and considered why each of the differences is present.
The code used in our comparison can be found in
Appendix~\ref{example-c-code-appendix}.

In what follows, we describe each of the examples and discuss points
of our translation to C code that are particularly relevant to each
example.
In Section~\ref{code-comparison-subsection}, we include a general
discussion of the similarities and differences observed while
comparing the code generated by our prototype to that generated by
icecap for the program methods of each example.
}

\subsection{\texorpdfstring{\texttt{PersistentSignal}}{PersistentSignal}}
\label{persistent-signal-subsection}

Our first example is \texttt{PersistentSignal}. 
It consists of a single mission with two event handlers:~a periodic
handler, \texttt{Producer}, and an aperiodic handler, \texttt{Worker}.
These communicate through an instance of a third class,
\texttt{PersistentSignal}, after which the example is named.
The \texttt{PersistentSignal} class contains a boolean flag, with
\texttt{synchronized} methods to read, set and clear it.
\texttt{Producer} releases clear the \texttt{PersistentSignal}
flag\deleted{,} and then signal\deleted{s} for the \texttt{Worker} to
release.
The \texttt{Worker} sets the \texttt{PersistentSignal} flag during its
release and the \texttt{Producer} checks the flag to see if the
\texttt{Worker} has finished its release.
Both the \texttt{Producer} and \texttt{Worker} produce output to
indicate when they are released.

The main purpose of this example is to demonstrate SCJ's scheduling
behaviour.
The priority of the \texttt{Worker} is set higher than the priority of
the \texttt{Producer}, so the \texttt{Worker} always preempts the
\texttt{Producer}, leaving the flag set at the end of its release
before allowing the \texttt{Producer} to finish its release.
This means that the synchronisation applied to the methods of
\texttt{PersistentSignal} may not be necessary, but it is good
practice due to the possibility of release jitter whereby the
scheduler may switch to a thread after a small delay if, for example,
the scheduler is running on its own thread or in response to clock
interrupts.

The code generated by our prototype for this example is similar to
that generated for it by icecap.
The operations on local variables and the operand stack are
represented by operations on C variables in both the code icecap
generates and the code resulting from our strategy. 
The names of the variables differ between icecap and our
implementation, since icecap uses the local variable names from the
original SCJ code, which are included in class files for debug
purposes, but different names can be used without affecting the
correctness of the code.

The synchronisation behaviour is particularly evident in this example,
and is handled the same in both the code from icecap and the code from
our prototype.
The lock is taken just before a call to a synchronized method, and
released at the end of the synchronized method.
In our model this is represented by the $takeLock$ and $releaseLock$
channels; icecap uses a \texttt{handleMonitorEnterExit()} function to
handle both, passing a boolean flag to it to distinguish between taking
the lock and releasing the lock.
The objects locked on are the same in our code as for icecap:~the
first argument on the stack when calling the method, and the first
local variable when returning from the method.

\addchange{Removed text explaining general differences between our
  code and icecap, since it is stated later}
\deleted{
  There are some differences, most of which
  apply to all the examples discussed in this section.
  The code from icecap has extra code to throw and handle exceptions,
  which we do not consider in our model since SCJ code can be checked
  to ensure exceptions will not be thrown.
  Handling of exceptions in our strategy is a possibility for future
  work, although it is not necessary for any of our examples.
}

\deleted{
Jumps in the bytecode are translated by icecap using \texttt{goto}
statements in C.
This allows bytecode instructions to be translated more directly, but
it means the resulting code is not fully MISRA-C compliant.
In our compilation strategy, we avoid the use of \texttt{goto} by
considering the control-flow graph of each method and introducing
structures such as loops and conditionals.
This has the added advantage of making the resulting code more
readable and, since we have have certainty that this transformation
does not change the semantics of the code, it is not necessary to use
the most direct translation as icecap does.
}

\deleted{
The icecap code uses many additional variables, most of which are used
to temporarily hold values indicating whether an exception has been
thrown.
It also has a stack pointer variable, which is passed to each method
(in addition to the method's arguments) and used, in addition to the
other variables, to hold some values so that the compiled code can be
mixed with interpreted code.
As this feature is not required in our work, we do not include a stack
pointer in the code generated by our compilation strategy.
The cases where extra variables are eliminated in our strategy are
matched by icecap, such as comparing variables representing stack
slots directly in conditionals, rather than copying them into
intermediate variables, which are removed by
Rule~[\nameref{eliminate-value1-value2-conditional-rule}] in our
strategy.
}

\subsection{\texorpdfstring{\texttt{Buffer}}{Buffer}}
\label{buffer-subsection}

Our second example is \texttt{Buffer}, which, like the previous
example, consists of two event handlers:~a periodic handler,
\texttt{Producer}, and an aperiodic handler, \texttt{Consumer}.
During a release of \texttt{Producer}, it calls the
\texttt{executeInOuterArea()} method of \texttt{ManagedMemory},
passing in an anonymous \texttt{Runnable} object stored in a field
\texttt{\_switch} of \texttt{Producer}.
The \texttt{run()} method of the object in \texttt{\_switch} allocates
an instance of \texttt{Object} and stores it in a field \texttt{data}
of \texttt{Producer}.
Since it is executed via \texttt{executeInOuterArea()}, this instance
of \texttt{Object} is allocated in the memory area outside the
per-release memory for \texttt{Producer}, which is the mission memory.
The object in \texttt{data} is then stored in a buffer and the
\texttt{Consumer} is released, which pops the object from the buffer.

The purpose of this example is to demonstrate the memory behaviour of
SCJ.
Since the object passed via the buffer is used by both event handlers,
it must be allocated in mission memory to ensure that it is available
to both event handlers.
Since objects are, by default, allocated in an event handler's
per-release memory area during its release, this allocation must be
performed via \texttt{executeInOuterArea()}.
The buffer itself must also be allocated in mission memory, but it
does not require use of \texttt{executeInOuterArea()}, since it is
allocated during mission initialisation.
Since the mission memory is not cleared during the mission, it would
eventually run out of space to allocate the objects with repeated
releases of \texttt{Producer}.
To prevent this, the \texttt{Producer} maintains a count of how many
times it has been released and does not allocate the object or store
it in the buffer if it has been released more than a set number of
times.

\addchange{Remove mention of characteristics described for
  \texttt{PersistentSignal}, since the discussion has been moved
  later}
\deleted{
The code generated from this example has many of the same
characteristics we have already described for the
\texttt{PersistentSignal} example.
In particular, the methods of the buffer are synchronized, so
synchronisations are produced in the code around calls to such
methods.
In addition, we make some further considerations related to the how
the use of the \texttt{executeInOuterArea()} method is represented in
the code.
}
\added{
Of note is how the use of the \texttt{executeInOuterArea()} method is
represented in the code, since it provides a good example of how
method calls are translated.
}
The call to \texttt{executeInOuterArea()} itself is a simple static
method call in both the code generated by icecap and the code
generated by our strategy, since the SCJ API does not differ between
them.
Although the implementation of the method differs between the icecap
code and the code from our strategy, due to the differing SCJ
libraries used, they both contain code to change the memory area, a
call to the \texttt{run()} method of the \texttt{Runnable} object
passed into the method, and code to change the memory area back to its
previous value.

The call to the \texttt{run()} method of the \texttt{Runnable} object
is interesting as many classes in the SCJ infrastructure implement
\texttt{Runnable}, providing a large set of possible targets for the
call.
There is a large difference in the set of targets chosen for the
method call by icecap and our prototype --- the icecap code lists 10
targets, whereas our code lists 4.
The only target that appears on both lists is \texttt{Producer\$1},
the anonymous class in \texttt{Producer} that is the actual target of
the \texttt{executeInOuterArea()} call we are considering.
The other three targets in our code are all subclasses of
\texttt{AsyncEventHandler}, which is part of the superclass hierarchy
for event handlers. 

\texttt{AsyncEventHandler} is included in the list of choices for
icecap and is selected there by searching the superclasses of the
object the method is called on until one of the listed targets is
found.
In our code, we adopt a different approach, selecting using the
object's actual type but directing the call to the class in which the
method is defined.
This means that while there are three branches of the choice
corresponding to subclasses of \texttt{AsyncEventHandler}, the
contents of those branches are the same call to
\texttt{AsyncEventHandler}'s \texttt{run()} method.
This is simply a static resolution of the superclass search that
icecap conducts, for each of the possible subclasses of
\texttt{AsyncEventHandler} in the example program, so it is
equivalent.
The other targets listed in the icecap code are parts of the SCJ
infrastructure that are handled in our model by the $Launcher$ and
SCJVM services.

%TODO: say something about how targets are chosen here?

\subsection{\texorpdfstring{\texttt{Barrier}}{Barrier}}
\label{barrier-subsection}

Our third example is \texttt{Barrier}, which demonstrates a common
pattern in real-time systems, where an event only happens when
multiple event handlers have signalled their readiness.
It is based around a class named \texttt{Barrier}, which implements
this pattern.
There are three types of event handlers in this
example:~\texttt{FireHandler}, which is the type of aperiodic event
handlers that must signal their their readiness to the
\texttt{Barrier}, \texttt{LaunchHandler}, the aperiodic handler that
releases when all the \texttt{FireHandler}s have signalled their
readiness, and \texttt{Button}, a periodic handler that simulates
events releasing the \texttt{FireHandler}s.
In the example, two instances of \texttt{FireHandler} are created,
with corresponding \texttt{Button} event handlers:~one that releases
every 2 seconds, and one that releases every 9 seconds.

When a \texttt{FireHandler} releases, it checks if it has already
triggered the \texttt{Barrier}, and calls a method of the
\texttt{Barrier} to trigger it if it has not already been triggered,
passing a numerical identifier.
When the \texttt{Barrier} is triggered, it sets a boolean flag
corresponding to the passed numerical identifier, then checks if all
the boolean flags are set.
When all the boolean flags are set, the \texttt{Barrier} releases its
associated \texttt{LaunchHandler} object and resets all the boolean
flags.
The \texttt{LaunchHandler} gives output to indicate when it is
released.

This example shows a more complex scheduling behaviour than that of
the previous examples.
The \texttt{FireHandler} and \texttt{LaunchHandler} event handlers
have higher priority than the \texttt{Button} event handlers, so they
cannot be interrupted by the periodic events firing.
However, the \texttt{FireHandler} and \texttt{LaunchHandler} handlers
have the same priority, and the methods of \texttt{Barrier} are
synchronized, so the boolean flags in \texttt{Barrier} are completely
reset before the \texttt{LaunchHandler} executes.
Due to the differing periods for the \texttt{Button} handlers, the
first \texttt{FireHandler} releases four or five times before the second
\texttt{FireHandler} releases and \texttt{LaunchHandler} executes.

A particular feature of interest in the code generated for this
example stems from the fact that it has multiple aperiodic event
handler types.
This means that a release of an aperiodic handler in our code (as may
occur when the \texttt{Barrier} is triggered) is represented by a
choice between them, although both branches of the choice contain a
call to the \texttt{release()} method of
\texttt{AperiodicEventHandler}.
The corresponding icecap code simplifies this to a direct call to the
\texttt{release()} method of \texttt{AperiodicEventHandler}, omitting
the unnecessary choice.
Such a transformation could be made in our strategy, although fully
applying it involves eliminating the $getClassIDOf$ communication used
to get the class identifier used in the choice.
As explained in
Section~\ref{compilation-final-considerations-section}, this requires
operating on multiple processes and so we leave it to future work.
% This is an example of the case discussed earlier where we cannot
% eliminate the choice in our code without operating on multiple
% processes, so applying such a simplification in our strategy is left
% to future work.
Note that the fact that there are multiple instances of
\texttt{Button} and \texttt{FireHandler} makes no difference to either
the code from icecap or the code from our strategy.

\addchange{Added subsection describing the similarities and differences
  observed when comparing code}
\added{
\subsection{Code comparison}
\label{code-comparison-subsection}

We observed various similarities between our code and that generated
by icecap.
Firstly, variables are generated to store the contents of stack slots
in both, with values being pushed to the stack by assignment to these
variables, and operations performed upon them.
Local variables of a method are also represented by C variables, and
arguments of the method are passed as arguments of the corresponding C
function in both our code and icecap's code.
There are some differences in the names of variables; we name
variables using \texttt{var} and a number while icecap uses the name
of the variable from Java. 
In addition, icecap distinguishes stack slots for different types,
although the basic approach is the same.

Method calls also display similarities, particularly for non-virtual
method calls, which are simple C function calls in both our code and
icecap's code.
Virtual method calls display some differences in how the method to be
called is selected (discussed below), but the method call itself is as
in the non-virtual case.
Calls to \texttt{synchronized} methods are also compiled in the same
way, with the lock being taken on an object before the method is
called and released just before the method ends, where the operations
of taking and releasing locks are performed by calls to infrastructure
functions.

We have also observed that field accesses are the same in both our
code and in icecap.
The fields are accessed in our code by casting a variable storing the
pointer to the object, first to \texttt{uintptr\_t} to ensure it is
expanded correctly on systems where pointers are wider than 32 bits
(since a single variable is a 32-bit integer in our code), and then to
a pointer to the struct type for the object's class, and by finally
accessing the field via a C field access.
The icecap code performs the access in the same way, although an
intermediate variable is used, a custom \texttt{pointer} type is used
in place of \texttt{uintptr\_t}, a few other pointer casts are applied
before the final cast to the class struct, and an optional memory
offset is allowed for.
The intermediate variable does not affect the semantics of the access,
nor do the additional pointer casts, and it is not clear why they are
present in the icecap code since they are not necessary. 
The \texttt{pointer} type is defined to be equivalent to the C99
\texttt{uintptr\_t} type (although it handles those compilers that may
not support C99), so there is no difference in the semantics of our
code and the icecap code on this point.
For the memory offset, we assume memory addresses used by the memory
manager are small enough to fit in a 32-bit JVM word.
Future work could add an offset to handle heaps outside that range if
necessary.

There are various differences between our code and icecap; many
of these relate to areas we have explicitly not considered in our
strategy.
In other cases, we have chosen to diverge from icecap's approach.
These differences are discussed next.

Firstly, there are several methods in our code that do not have
corresponding methods in the code generated by icecap.
This is due to a combination of different factors.
Some methods are not present in the code input to icecap code due to
differences in the version of the SCJ API used by our code and icecap.
Other methods are present in the code input to both our prototype and
icecap, yet have no C code generated for them in icecap.
The lack of code for these methods in icecap is due to a difference
between our prototype and icecap in how the set of methods to be
compiled is computed.
Our prototype generates C code for all methods passed to it, whereas
icecap computes which methods are required for the program, beginning
from the main method that forms the starting point of the launcher.
While this does exclude one method
(\texttt{main\_BoundedBuffer\_isFull}) that is defined in the example
code but not used, the other methods have no corresponding icecap code
due to the fact that they appear not to be called in icecap's launcher
infrastructure.
This would appear to be a deficiency in icecap's implementation of the
SCJ startup procedure, where fixed sizes are used for the immortal and
mission memory rather than obtaining them from the \texttt{Safelet}
and \texttt{Mission} provided by the program.

Another difference is that the icecap code passes a frame pointer,
\texttt{fp}, to each function and defines a stack pointer variable,
\texttt{sp}, in each function.
These are used to manage a stack, which is used in addition to the
stack slot variables.
This stack allows the compiled code to interact with interpreted code,
since the interpreted code uses this stack rather than having
predefined stack slots (which are computed during the compilation
process).
We do not require this feature in our code, since all our code is
compiled.
For the same reason, we also do not generate the code to swap stack
slot variables to and from this stack.
There are also some infrastructure methods in icecap that accept their
arguments using the stack, and a few of the generated functions in the
icecap code (such as \texttt{main\_BoundedBuffer\_init}) pop their
arguments from the stack rather than taking them as function
arguments.
This is, of course, unnecessary for our code, where we adopt the same
approach of passing arguments as C function arguments for all methods.

The icecap code also uses a different approach for returning values
from functions.
In the icecap code, return values are passed using the stack passed
into the function, with the return value popped from the stack in the
calling function.
In our code, we do not pass a stack pointer, so pointers to the stack
slot variables in which the return values are to be placed are passed
instead.
This approach used in our code is preferable to using C return values
as it scales better to \texttt{long} values, which require two
variables.
We note that icecap functions returning small values, particularly
\texttt{boolean} values in the case of our examples, instead use the
\texttt{int16} value returned from each function (normally used to
signal exceptions in the icecap code) to pass the return value.
This is a somewhat inconsistent approach to passing the return values,
but it perhaps makes best use of space for small values.

There is a lot of exception handling code in icecap that is not
present in our code, since we do not handle exceptions.
% The extra code that is present in icecap includes extra varibles to
% hold exception information, checks for null pointers before array
% accesses and method calls, checks after method calls to determine if
% an exception was thrown by the method, exception handler code at the
% end of the function, and return values communicated to indicate if an
% exception has been thrown.
As we have already seen, the return values for signalling exceptions
are also used to pass small return values of the method.
We omit the return values completely in our code, since we do not need
to signal exceptions, but our system of passing method return values
frees up the function return value for use as part of an exception
handling system in future work.

There is also a difference in how control flow constructs are
compiled.
Jumps in the bytecode are translated by icecap using \texttt{goto}
statements in C.
This allows bytecode instructions to be translated more directly, but
it means the resulting code is not fully MISRA-C compliant.
In our compilation strategy, we avoid the use of \texttt{goto} by
considering the control-flow graph of each method and introducing
structures such as loops and conditionals.
This has the added advantage of making the resulting code more
readable and, since we have have certainty that this transformation
does not change the semantics of the code, it is not necessary to use
the most direct translation as icecap does.

Differences in the code arising from the different API provided for
our code versus icecap also show themselves in the generated code.
Arrays operations in icecap are translated using C array accesses but,
since we do not have arrays, we model them as objects.
It is expected that future work that adds handling of arrays to our
compilation strategy would produce code similar to that of icecap.
There are also some places where \texttt{static} fields holding
constant values for memory sizes are additionally declared
\texttt{final} in our code, where in icecap they merely hold the same
value as a \texttt{final} field.
This means accesses to these fields appear in the icecap code, but the
values of the fields are inlined in our code.
However, from \texttt{static} field accesses in our SCJ API
implementation (such as in \texttt{devices\_Console\_read} for each of
the examples), we see that they are translated in the same way in our
code as in icecap:~by an access to a field of a global struct
containing class fields.

Finally, there is also a difference in how virtual method call targets
are chosen in our code versus that of icecap.
In icecap, the superclasses of the target object are searched to
determine which method should be executed, whereas in our code a
choice is made over the class of the target object, with the
superclasses searched at compile time.
Our approach means the work of searching for the class containing the
definition need not be performed at runtime, but results in several
conditional branches with the same body for classes that have a common
superclass.
As a future optimisation in our code, such branches could be merged
and a switch statement could be used for a more efficient choice over
classes.
The icecap code also removes the search entirely if there is only a
single possible target.
Such a transformation could be made in our strategy, although fully
applying it involves eliminating the $getClassIDOf$ communication used
to get the class identifier used in the choice.
As explained in
Section~\ref{compilation-final-considerations-section}, this requires
operating on multiple processes and so we leave it to future work.
In any case, the different approaches to selecting the target method
yield the same target at run-time.

We also noted a difference in the size of the code generated by our
prototype versus that generated by icecap.
Our prototype generates two files for each example:~a \texttt{.c} file
containing the code for each of the methods, and a \texttt{.h} file
containing struct definitions and prototypes for infrastructure
functions (the provision of which is outside the scope of this
thesis).
The files generated by icecap include many pre-defined files, that do
not result from compilation of the examples.
Namely, the files that are generated by icecap from the code of each
of the examples are a \texttt{methods.c} file, containing the code of
each method, a \texttt{methods.h} file, defining constants used to
identify each of the methods, a \texttt{classes.c} file, defining
variables containing class information, and a \texttt{classes.h} file,
defining struct types for the objects of each class.
The \texttt{methods.c} file corresponds to the \texttt{.c} file
generated by our prototype, and the \texttt{classes.h} file
corresponds to the \texttt{.h} file generated by our prototype. 
So we compare the sizes of those files.
The class information in \texttt{classes.c} and the method identifiers
in \texttt{methods.h} are included in icecap to support interpretation
of bytecode, which is not necessary in our code, so we do not include
these files in our comparison.

The sizes of the \texttt{.c} files are:~2576 lines for
\texttt{PersistentSignal}, 2748 lines for \texttt{Buffer}, and 2787
lines \texttt{Barrier}.
The sizes of the corresponding \texttt{methods.c} files generated by
icecap are~63968 lines for \texttt{PersistentSignal}, 65383 lines for
\texttt{Buffer}, and 64619 lines for \texttt{Barrier}.
The sizes of the \texttt{.h} files are:~542 lines for
\texttt{PersistentSignal}, 560 lines for \texttt{Buffer}, and 551
lines \texttt{Barrier}.
The sizes of the corresponding \texttt{classes.h} files generated by
icecap are~1164 lines for \texttt{PersistentSignal}, 1187 lines for
\texttt{Buffer}, and 1181 lines for \texttt{Barrier}.
Note that these sizes are the size of the complete files, including
blank lines and comments.

The icecap files are clearly much larger, but this includes the larger
SCJ API implementation of icecap.
Extracting the definitions of each of the program methods from the
\texttt{.c} files generated by the prototype gives 362 lines for
\texttt{PersistentSignal}, 508 lines for \texttt{Buffer}, and 547
lines for \texttt{Barrier}.
Similarly, extracting the program method definitions from the
\texttt{methods.c} files generated by icecap gives 1634 lines for
\texttt{PersistentSignal}, 2041 lines for \texttt{Buffer}, and 2241
lines for \texttt{Barrier}.
The difference in size for the program methods is smaller, but the
size of the icecap method code is still more than four times the size
of the code generated by our prototype for each example.
This is, however, largely accounted for by the fact that icecap
includes extra code for exception handling and comments indicating
which line of the original Java code each line of C code corresponds
to.
For both our prototype and icecap, the C code is longer than the
original Java files, the sizes of which are: 270 lines for
\texttt{PersistentSignal}, 343 lines for \texttt{Buffer}, and 318
lines for \texttt{Barrier}.
The input to icecap also includes an additional 10-line file
containing a \texttt{main()} method that invokes the icecap launcher
infrastructure, and may be taken as part of the $Launcher$ in our
model.
The longer size for the C code over the Java code follows from the
fact that each line of Java code may be translated by multiple
bytecode instructions.

Overall, our code is similar to that of icecap; differences are
justified in that they are more suited to the particular approach we
adopt:~not interpreting and ensuring MISRA-C compliance of the code.
This thus provides additional confidence in the validity of the code
generated by our strategy.
}

\section{Final Considerations}
\label{evaluation-final-considerations-section}

In this chapter, we have considered various ways in which our model
and compilation strategy can be evaluated, and their correctness
validated.
The models used as input to the strategy have been validated by using
CZT to perform syntax and type checking, and performing some proofs
using Z/EVES on the schemas defining instruction semantics.
We have seen that this ensures that the model is well-formed and
provides a means to deduce the preconditions that must be satisfied
for each bytecode instruction.
The preconditions found match those checked by JVM bytecode
verification, ensuring our semantics is correct for standard Java
bytecode.

We have also discussed the proofs of the compilation rules and the
source of the laws used used to prove those rules, seeing that the
algebraic proof style of the rules gives great certainty of the
proof's correctness by formally justifying each step in the proof.
As we mentioned, the laws we have used come from existing \Circus{}
laws taken from various sources and laws we have proved in
Isabelle/UTP.
This basis of laws known to be correct provides further assurance of
the correctness of the proofs.

Finally, we have discussed our prototype implementation of the
compilation strategy and the assurance that may be gained from
considering some examples.
The tool shows how the individual compilation rules fit together as a
complete whole, allowing us to check how the rules act as part of the
strategy.
The examples we have considered show that the code we generate is
generally comparable to that generated by icecap.
The few differences observed between our code and icecap's code arise
from design choices that enhance the generated code.

\addchange{Added discussion of execution time and complexity of prototype}
\added{
While we have used the prototype to check the form of the generated
code, it can also give us an idea of the complexity of the strategy so
that we can judge how viable it would be were we to make a full
implementation of it.
By packaging the prototype as a \texttt{.jar} file we can execute it
from a command line and use the \texttt{time} command to measure its
execution time.
We have performed this in Ubuntu 18.04 running on an Intel Core
i5-520M processor.
Averaging wall-clock time across 10 runs of the prototype for each of
our examples yields 2.50 seconds for \texttt{PersistentSignal}, 2.78
seconds for \texttt{Barrier}, and 2.67 seconds for \texttt{Buffer}.

From the output of the prototype indicating which stages of the
strategy are executing, the bulk of the time appears to be spent in
introducing sequential composition.
More detailed tracing of the time taken shows that this is due to the
fact that the control flow graph is reconstructed after each
compilation rule is applied.
The number of reachable nodes is very high at the start of the
compilation strategy, but reduces by a large amount during sequential
composition introduction, since most of the edges between nodes
represent sequential composition.
This time could be reduced by using a more sophisticated strategy to
perform local updates of the control flow graph, potentially reducing
the execution time of the prototype by up to 2 seconds.
With other optimisations, such as more efficient data structures and
pattern matching strategies, this could give reasonable execution
time, even for large programs.

It is difficult to produce similar measurements of compilation time
for icecap, since icecap is designed as an Eclipse plugin and cannot
be separated from Eclipse to allow measurement of compilation time.
However, we note that the compilation time for our examples in icecap
seems to be of the same order of magnitude as for our prototype.

Our implementation is just a prototype and so any measurements of its
running time are only approximations of the efficiency of our
compilation strategy.
It is more helpful to consider the asymptotic complexity of the
compilation strategy, to determine if it scales well in an optimised
implementation.
Assuming an input program consists of $m$ methods containing an
average of $n$ instructions each, and that the local updates of the
control flow graph are made to take constant time, then the time
complexity of our strategy is at most $\mathcal{O}(m^3n)$.
This is because, firstly, the loop on line~\ref{algorithm-method-loop}
of Algorithm~\ref{epc-algorithm} may loop once for each of the $m$
methods if only one method is separated in each iteration.
At least one method will be separated in each iteration and it is
expected that more than one will be separated in most iterations, so
$m$ is a conservative upper bound on the number of iterations in the
loop.
Within the loop, Algorithm~\ref{resolve-method-calls-algorithm} checks
each method call instruction, of which there may be up to $mn$, and
for each target of the method call, of which there could be as many as
there are methods, $m$, searches its superclasses for an
implementation, of which there may be as many as there are methods,
$m$.

None of the other algorithms contributes as much as
Algorithm~\ref{resolve-method-calls-algorithm} to the time complexity,
since all the other elimination of program counter algorithms simply
iterate over each node in at most one loop. 
Even Algorithm~\ref{introduce-forward-sequence-algorithm}, which has
two nested loops, is linear in the number of instructions, since any
additional iteration of the inner loop means a sequential composition
is introduced so the nodes are merged. 
Thus there is one fewer node and one fewer iteration of the outer
loop.

The elimination of frame stack algorithms are generally at most
$\mathcal{O}(m^2n)$, since they may transform each call of the
potential $mn$ calls to the $m$ methods.
The data refinement of objects has a separate complexity, since it is
determined by the number of classes and fields. 
It is unlikely to contribute more to complexity than the instructions,
since each field should be accessed by at least one instruction.

The overall complexity is thus $\mathcal{O}(m^3n)$, but this is an
upper bound and in most cases iteration will not be over all methods.
It may be possible to find iteration strategies to reduce this
asymptotic complexity by iterating over the methods fewer times.
This can lead to the strategy being quadratic or linear in $m$, which
seems an acceptable complexity.
}

All these considerations serve to validate the correctness of the
model and strategy, and shows that our strategy is a viable
\added{scalable} basis for a correct-by-construction ahead-of-time
SCJ-to-C compiler.
