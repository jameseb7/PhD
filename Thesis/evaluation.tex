\chapter{Evaluation}
\label{evaluation-chapter}

In this chapter, we evaluate our model and compilation strategy, using
several approaches.
First, we consider what assurances can be gained from mechanisation of
the model and proofs of the compilation rules.
In addition, we compare code produced by our strategy to that produced by
icecap, using some examples to evaluate the strategy.
% TODO: more that we can add here?
We note, finally, that the process of constructing the model already
embeds important validation effort, via numerous reviews of the
standard, and close interaction with the standardisation committee,
which led to some changes to the standard.

Next, in Section~\ref{mechanisation-of-models-section}, we consider
the assurances gained from mechanisation of the models that form the
starting point of the strategy.
In Section~\ref{proofs-of-laws-section}, we discuss the proofs of the
compilation rules used in the strategy and how they provide assurances
of the correctness of the strategy.
Afterwards, in Section~\ref{tool-support-section}, we consider
mechanisation of the strategy and then, in
Section~\ref{examples-section}, we evaluate the strategy with some
examples.
Finally, we conclude in
Section~\ref{evaluation-final-considerations-section}.


\section{Mechanisation of Models}
\label{mechanisation-of-models-section}

% TODO: rephrase this
The correctness of our compilation strategy relies on the correctness
of the models used as input to the compilation strategy.
Their correctness relies on the inputs to the models meeting the
assumptions made in Section~\ref{compilation-assumptions-section}.
If these assumptions are not met, then the behaviour of model is not
correct and the compilation strategy cannot be applied.
For example, if the sequence of instructions in the program causes the
operand stack to overflow the maximum stack size, the invariant of
$StackFrame$ is violated and program's behaviour is chaotic.
Our compilation strategy cannot be applied to such a program, since no
stack slots are created beyond the maximum stack size to handle such a
situation in the strategy, and it is not clear what the expected C
code would be.

As discussed in Section~\ref{cee-validation-section}, the fact that
the models are written in CZT ensures they have correct syntax and
types.
CZT performs this checking continuously and flags up errors as they
occur, so they can be quickly corrected during the writing of the
models.

We have also performed some proofs on the Z schemas defining the
semantics of the bytecode instructions, using Z/EVES 2.4.1 with CZT as
its user interface.
There are two main groups of results.
The first is domain check proofs, ensuring partial functions are
not applied outside their domain.
These are proof obligations generated by Z/EVES, and so do not have
corresponding theorems stated.
These proofs are not required for schemas that do not directly
reference partial functions.

The second group of results is precondition proofs.
These require that a final state exists for the schema, which ensures
that the requirements of the schema are not contradictory.
Stating and proving these theorems also extracts the preconditions of
the operations, since those must be stated as assumptions of the
theorems.

The preconditions we have found include those required to avoid
operand stack overflows and underflows, that local variable indices
are within the range of the local variable array, and that
program-address updates do not go outside of the current method's
bytecode array.
These conditions are ensured by standard JVM bytecode verification,
which we assume inputs to the strategy pass.
The existence of at least one stack frame is also required for
bytecode instructions to execute, and this property is ensured by the
condition on the loop in the $Running$ action.

A further precondition required by the interpreter operations is that
the value $cs$ is such that the class and method in which a program
address occurs is unique.
This condition is required to ensure that the current class and method
can be uniquely determined from the value of $pc$.
This is required by the invariant of $InterpreterState$, but need only
be fulfilled as a precondition when a new stack frame is created,
since it can be ensured from the invariant on the initial state for
the other operations.
This condition on $cs$ is reasonable since the bytecode instructions
for each method should be at separate addresses in $bc$.

% TODO: these proofs will be removed in the main thesis
The statements of the theorems proved can be found in
Appendix~\ref{stack-frames-theorems-appendix} and
Appendix~\ref{interpreter-theorems-appendix}, with their corresponding
proofs in Appendix~\ref{stack-frames-proofs-appendix} and
Appendix~\ref{interpreter-proofs-appendix}.
We have also proved various additional lemmas in the course of
constructing these proofs.
Those which are specific to the model are listed along with the
precondition theorems in Appendix~\ref{stack-frames-theorems-appendix}
and Appendix~\ref{interpreter-theorems-appendix}.
Some of them are general facts that could be of use in other theorems,
which are listed in Appendix~\ref{additional-lemmas}.

\section{Proofs of Laws}
\label{proofs-of-laws-section}

% discuss compilation rules and their proofs
% explain the importance of the algebraic proof style
% explain source of laws used for proofs

\section{Tool Support}
\label{tool-support-section}

% discuss mechanisation of strategy
% explain considerations surrounding bytecode subset
% explain what confidence this brings

\section{Examples}
\label{examples-section}

% illustrate with some examples
% compare to icecap

\section{Final Considerations}
\label{evaluation-final-considerations-section}